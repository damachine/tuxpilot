# üêß TuxPilot + Ollama Setup Guide

**Lokale AI ohne Cloud-Abh√§ngigkeit und API-Kosten!**

## üöÄ **Schnell-Setup (Automatisch)**

```bash
# Alles automatisch installieren und konfigurieren
./setup-ollama.sh
```

Das Script macht alles f√ºr dich:
- ‚úÖ Ollama installieren
- ‚úÖ Service starten  
- ‚úÖ Modell herunterladen
- ‚úÖ TuxPilot konfigurieren
- ‚úÖ Kompilieren und testen

## üîß **Manuelles Setup**

### **1. Ollama installieren**

```bash
# Ollama herunterladen und installieren
curl -fsSL https://ollama.ai/install.sh | sh

# Pr√ºfen ob installiert
ollama --version
```

### **2. Ollama Service starten**

```bash
# Service im Hintergrund starten
ollama serve &

# Oder als systemd Service (empfohlen)
sudo systemctl enable ollama
sudo systemctl start ollama
```

### **3. AI-Modell herunterladen**

```bash
# Empfohlenes Modell (ca. 4.7 GB)
ollama pull llama3.1:8b

# Weitere Optionen:
ollama pull mistral:7b        # Schneller, kleiner
ollama pull codellama:7b      # Speziell f√ºr Code
ollama pull llama3.1:70b      # Sehr gut, braucht viel RAM (40+ GB)

# Verf√ºgbare Modelle anzeigen
ollama list
```

### **4. TuxPilot konfigurieren**

```bash
# Konfigurationsverzeichnis erstellen
mkdir -p ~/.config/tuxpilot

# Ollama-Konfiguration kopieren
cp examples/ollama-config.toml ~/.config/tuxpilot/config.toml

# Oder manuell bearbeiten
nano ~/.config/tuxpilot/config.toml
```

**Beispiel-Konfiguration:**
```toml
[ai]
provider = "Ollama"

[ai.ollama]
base_url = "http://localhost:11434"
model = "llama3.1:8b"
temperature = 0.7
context_size = 4096
timeout_seconds = 30
```

### **5. TuxPilot kompilieren**

```bash
cargo build --release
```

## üéØ **Verwendung**

### **Grundlegende Befehle:**

```bash
# Interaktiver Chat-Modus (komplett offline!)
./target/release/tuxpilot chat

# Paket-Management Hilfe
./target/release/tuxpilot package install firefox

# Automatische Fehlerdiagnose
./target/release/tuxpilot diagnose --auto

# System-Monitoring mit AI-Analyse
./target/release/tuxpilot monitor

# Befehl erkl√§ren lassen
./target/release/tuxpilot explain systemctl
```

### **Chat-Beispiele:**

```
tuxpilot> Mein System ist langsam, was kann ich tun?
ü§ñ TuxPilot: Ich helfe dir bei der Performance-Analyse...

tuxpilot> Wie installiere ich Docker auf Arch Linux?
ü§ñ TuxPilot: F√ºr Docker auf Arch Linux verwendest du...

tuxpilot> nginx startet nicht, was ist das Problem?
ü§ñ TuxPilot: Lass mich den nginx Service analysieren...
```

## ‚öôÔ∏è **Konfiguration**

### **Modell wechseln:**

```bash
# Anderes Modell herunterladen
ollama pull mistral:7b

# In config.toml √§ndern
[ai.ollama]
model = "mistral:7b"
```

### **Performance optimieren:**

```toml
[ai.ollama]
# F√ºr schnellere Antworten
temperature = 0.3
context_size = 2048
timeout_seconds = 15

# F√ºr bessere Qualit√§t
temperature = 0.8
context_size = 8192
timeout_seconds = 60
```

### **Remote Ollama Server:**

```toml
[ai.ollama]
base_url = "http://192.168.1.100:11434"  # Anderer Server
model = "llama3.1:8b"
```

## üîç **Troubleshooting**

### **"Ollama API error" Fehler:**

```bash
# Pr√ºfen ob Ollama l√§uft
curl http://localhost:11434/api/tags

# Service neu starten
pkill ollama
ollama serve &

# Logs pr√ºfen
journalctl -u ollama -f
```

### **"Model not found" Fehler:**

```bash
# Verf√ºgbare Modelle anzeigen
ollama list

# Modell herunterladen
ollama pull llama3.1:8b

# In config.toml korrigieren
model = "llama3.1:8b"  # Exakter Name aus 'ollama list'
```

### **Langsame Antworten:**

```bash
# Kleineres Modell verwenden
ollama pull mistral:7b

# Oder Timeout erh√∂hen
timeout_seconds = 120
```

### **Speicher-Probleme:**

```bash
# Kleineres Modell verwenden
ollama pull llama3.1:8b    # statt 70b

# Oder Context reduzieren
context_size = 2048        # statt 4096
```

## üìä **Modell-Empfehlungen**

| Modell | Gr√∂√üe | RAM | Geschwindigkeit | Qualit√§t | Verwendung |
|--------|-------|-----|----------------|----------|------------|
| `mistral:7b` | 4.1 GB | 8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Schnelle Antworten |
| `llama3.1:8b` | 4.7 GB | 8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | **Empfohlen** |
| `codellama:7b` | 3.8 GB | 8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Code-Probleme |
| `llama3.1:70b` | 40 GB | 64 GB | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Beste Qualit√§t |

## üéâ **Vorteile von Ollama + TuxPilot**

‚úÖ **Komplett offline** - keine Internetverbindung n√∂tig
‚úÖ **Keine API-Kosten** - einmal installiert, immer kostenlos  
‚úÖ **Datenschutz** - alle Daten bleiben lokal
‚úÖ **Schnell** - keine Netzwerk-Latenz
‚úÖ **Anpassbar** - verschiedene Modelle f√ºr verschiedene Zwecke
‚úÖ **Zuverl√§ssig** - keine Rate-Limits oder Service-Ausf√§lle

## üîÑ **Systemd Service (Optional)**

F√ºr automatischen Start bei Boot:

```bash
# Service-Datei erstellen
sudo tee /etc/systemd/system/ollama.service << 'EOF'
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3

[Install]
WantedBy=default.target
EOF

# User erstellen
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# Service aktivieren
sudo systemctl enable ollama
sudo systemctl start ollama
```

**Jetzt hast du eine komplett lokale AI-L√∂sung! üöÄ**
